# Video | Apple Developer Documentation

> Source: https://developer.apple.com/documentation/technologyoverviews/video
> Fetched: 2025-08-31T18:32:57.740Z

To build a video playback app, a custom video camera, or a full-featured editing suite, use [AVFoundation](https://developer.apple.com/documentation/AVFoundation), Apple’s powerful framework for working with time-based media. It provides the foundation for most video experiences in iOS, macOS, tvOS, and visionOS—handling everything from high-performance playback, capture, export, and more. Combined with support for system features like AirPlay, Picture in Picture, and background playback, it enables you to create feature-rich apps that are intuitive and integrate seamlessly with the operating system.

## [Prepare video assets for playback](https://developer.apple.com/documentation/technologyoverviews/video#Prepare-video-assets-for-playback)

The video that your app plays can come in different formats, encodings, and locations. So that you can focus on delivering the functionality you want without without concern for these details, AVFoundation provides the [`AVAsset`](https://developer.apple.com/documentation/AVFoundation/AVAsset) class. You create an asset object by passing it a URL that represents the media you want to play, which can be a file-based content that exists on the local device, resides on a remote server, or is streamed over the internet. In all cases, the framework does the work to efficiently retrieve, load, and prepare the asset leaving you with a simple, uniform interface to inspect and play it.

Although Apple platforms provide support for several video formats, most video playback involves video encoded using H.264 (AVC) or H.265 (HEVC). This content can reside in container formats like QuickTime movies (`.mov`) and MPEG-4 files (`.mp4`, `.m4v`), or be delivered using HTTP Live Streaming (`.ts`, and `.fMP4`). Apple devices provide hardware-accelerated decoding of either format, but HEVC is the recommended choice for efficiently delivering and playing 4K, high-dynamic-range (HDR) video.

## [Play video content](https://developer.apple.com/documentation/technologyoverviews/video#Play-video-content)

Playing video is a great way to enhance your app’s user experience by delivering engaging content that captures people’s attention, communicates information effectively, and adds emotional impact. Whether you’re streaming full-length media, providing interactive tutorials, or incorporating visual elements into your user interface, video plays a crucial role in bringing your app to life. Using Apple’s powerful video frameworks like AVKit and AVFoundation makes it straightforward to add these features to your app.

You play video on Apple platforms using the AVFoundation framework’s [`AVPlayer`](https://developer.apple.com/documentation/AVFoundation/AVPlayer) class. A player object provides the interface to play audiovisual media assets, including local and remote file-based assets like [QuickTime](https://developer.apple.com/documentation/quicktime-file-format) movies and MPEG-4 video files, as well as media streamed using [HTTP Live Streaming](https://developer.apple.com/documentation/HTTP-Live-Streaming). It offers a comprehensive set of features for video playback, and requires only [minimal configuration](https://developer.apple.com/documentation/AVFoundation/configuring-your-app-for-media-playback) to take advantage of advanced system features like like AirPlay, Picture in Picture, and background audio playback. It even integrates seamlessly with the [Group Activities](https://developer.apple.com/documentation/GroupActivities) framework to support [SharePlay](https://developer.apple.com/design/human-interface-guidelines/shareplay) video experiences that enable people to watch together wherever they are.

A player object provides the interface to play media, but doesn’t present video onscreen or display a user interface to control playback. Instead, your app has the flexibility to display video in a way that best fits your desired experience. For example, it can play video in a system playback UI, like found in the TV and Music apps, or you can build an entirely custom interface that fits your use case and design. In either case, you typically use [AVKit](https://developer.apple.com/documentation/AVKit), which is a companion framework that provides user interface components and system integration support. How you use AVKit depends on your preferred playback user interface:

- **System player UI**: To support the system player user interface, adopt the [`AVPlayerViewController`](https://developer.apple.com/documentation/AVKit/AVPlayerViewController) (on iOS, tvOS, and visionOS) or [`AVPlayerView`](https://developer.apple.com/documentation/AVKit/AVPlayerView) (on macOS) classes. These classes are built on UIKit and AppKit, respectively, but are [easy to use](https://developer.apple.com/documentation/visionos/destination-video) in SwiftUI apps. Both classes offer intuitive, familiar playback controls and automatically support features like AirPlay, Picture in Picture, and remote playback from system interfaces like the Lock Screen and Control Center. Building your playback UI with these classes is the best approach for most apps because they deliver the expected experience across Apple platforms, integrate with essential system services, and automatically follow the latest Apple design conventions as the operating systems evolve.
- **Custom player UI**: If a custom user interface is a better fit for your app, you can implement it with your choice of [UI framework](https://developer.apple.com/documentation/technologyoverviews/app-design-and-ui) and adopt APIs from [AVKit](https://developer.apple.com/documentation/AVKit), [Media Player](https://developer.apple.com/documentation/MediaPlayer), and [Media Accessibility](https://developer.apple.com/documentation/MediaAccessibility) to provide seamless integration with system services. Use AVKit to support [Picture in Picture](https://developer.apple.com/documentation/AVKit/AVPictureInPictureController) and show the system [AirPlay picker](https://developer.apple.com/documentation/AVKit/AVRoutePickerView). Use Media Player framework to add [Now Playing](https://developer.apple.com/documentation/MediaPlayer/MPNowPlayingInfoCenter) information and support [remote control](https://developer.apple.com/documentation/MediaPlayer/MPRemoteCommandCenter) of your app from interfaces like the Lock Screen or Control Center. Finally, use Media Accessibility to align your presentation of subtitles and captions with peoples Accessibility preferences.

Your app can also present video outside of a typical player experience. For example, you can use [`AVPlayerLayer`](https://developer.apple.com/documentation/AVFoundation/AVPlayerLayer) to present a video splash screen or display a [looping](https://developer.apple.com/documentation/AVFoundation/AVPlayerLooper) animated background in your UI. You can even incorporate [video content as a material](https://developer.apple.com/documentation/RealityKit/VideoMaterial) that you display on top of a 3D model.

## [Capture video content](https://developer.apple.com/documentation/technologyoverviews/video#Capture-video-content)

If you’re interested in building a camera app, AVFoundation provides a rich set of features to capture video using built-in and external cameras. Camera apps are built around the framework’s [`AVCaptureSession`](https://developer.apple.com/documentation/AVFoundation/AVCaptureSession) class. You use a capture session to connect inputs from cameras and microphones, attach them to outputs like movie files and streams, and start and stop the capture pipeline. On all supported platforms, the framework provides a wide variety of capabilities that enable you to perform high-performance capture, provide realtime preview, and support precise control of camera hardware. On iPhone devices, AVFoundation supports additional features including capturing [Cinematic](https://developer.apple.com/documentation/Cinematic) video, that enables advanced cinematography effects, and [Spatial video](https://developer.apple.com/documentation/technologyoverviews/immersive-media) for playback on Apple Vision Pro.

When [building a camera app](https://developer.apple.com/documentation/AVFoundation/avcam-building-a-camera-app) for iPhone, a great way to enhance its experience is to seamlessly integrate it with system hardware and software:

- Support the [Camera Control](https://developer.apple.com/design/human-interface-guidelines/camera-control): This is a hardware button available on iPhone 16 and later models that you use to control common camera settings. Using AVFoundation, you can support the Camera Control to enable adjusting your camera’s features like zoom, exposure, and more directly from the hardware.
- Capture from hardware buttons: The AVKit framework provides the [`AVCaptureEventInteraction`](https://developer.apple.com/documentation/AVKit/AVCaptureEventInteraction) class to enable starting and stopping capture by pressing hardware buttons like the Camera Control or volume buttons.
- Launch your app from system interfaces: Adopt the [LockedCameraCapture](https://developer.apple.com/documentation/LockedCameraCapture) framework to enable launching your camera from the Lock Screen, Action Button, and Control Center.

If you would instead like to capture video from a device screen, Apple also offers robust screen capture capabilities available through the ReplayKit and ScreenCaptureKit frameworks. Which one is right for your app depends on your use case and required platform support:

- [ReplayKit](https://developer.apple.com/documentation/ReplayKit) enables you to add screen recording, live streaming, and in-app audio and video capture to your iOS, tvOS, and macOS apps and games. With minimal setup, you can record gameplay or app experiences, capture microphone audio and front-facing camera video for commentary, and share content directly from your app. ReplayKit supports both session recording and live broadcasting to streaming platforms, making it ideal for social sharing, game replays, and app demonstrations.
- [ScreenCaptureKit](https://developer.apple.com/documentation/ScreenCaptureKit) is designed specifically for high-performance and low-latency screen capture on macOS. It supports fine-grained control over capture sources, allowing you to record full displays, individual windows, or specific regions of the screen. Along with capturing screen content, you can also capture audio from an app or microphone. ScreenCaptureKit is optimized for efficiency, making it suitable for professional apps like streaming, real-time collaboration tools, and screen recording utilities.

Whether your app captures video from cameras or the screen, a great companion framework is [PhotoKit](https://developer.apple.com/documentation/PhotoKit). It provides the interface to save and interact with a person’s Photo Library, which serves as the central repository for all photos and videos on Apple devices. By adopting PhotoKit, you ensure that people can store captured videos in the same place they expect to find all their memories.

## [Edit video content](https://developer.apple.com/documentation/technologyoverviews/video#Edit-video-content)

AVFoundation provides a versatile set of APIs for editing time-based media, allowing you to build anything from simple trimming tools to professional-grade, nonlinear video editors. The editing features of AVFoundation are built around the `AVComposition` and `AVMovie` classes. While these classes provide some similar functionality, they’re intended to support different use cases:

- **[`AVComposition`](https://developer.apple.com/documentation/AVFoundation/AVComposition)** is designed to perform timeline-based editing that allows you to combine, trim, and reorder media from multiple sources. You can use it to assemble segments from audio and video tracks and put them together into a new asset that you can use to play or export. The framework also provides interfaces to specify compositing behavior to add video transitions like cross-dissolves, fades, and so on. Similarly, you can specify the how to mix assets that contain multiple audio tracks allowing you to apply audio fades, ducking, or use effects.
- **[`AVMovie`](https://developer.apple.com/documentation/AVFoundation/AVMovie)** provides support for direct editing of QuickTime files. It provides the interface for inspecting and working with movie file structures, tracks, and metadata. It allows you to load movies from disk or in-memory data, access individual media tracks (such as video, audio, or subtitle tracks), and read metadata across multiple formats. It’s ideal for apps that need to analyze, catalog, or extract information from movie files.

If you’re interested in building an app to integrate with Final Cut Pro, Motion, or Compressor, or extend their features, use the [Professional Video Applications](https://developer.apple.com/documentation/professional_video_applications) framework. This is a macOS framework that you use to exchange data with Final Cut Pro, create effects plug-ins for Final Cut Pro and Motion, and support custom output formats in Compressor.

## [Perform advanced video processing](https://developer.apple.com/documentation/technologyoverviews/video#Perform-advanced-video-processing)

In addition to the higher-level functionality that AVFoundation provides for working with timed media, it also provides lower-level [`AVAssetReader`](https://developer.apple.com/documentation/AVFoundation/AVAssetReader) and [`AVAssetWriter`](https://developer.apple.com/documentation/AVFoundation/AVAssetWriter) classes when you need to work directly with media samples. You can use these interfaces, along with the frameworks on which AVFoundation is built, to create advanced processing pipelines that support any video use case. These supporting frameworks include the following:

Framework

Description

[Core Media](https://developer.apple.com/documentation/CoreMedia)

Provides the foundation for working with timed media. Use the framework’s low-level data types and interfaces to efficiently process media samples and manage queues of media data.

[Core Video](https://developer.apple.com/documentation/CoreVideo)

Provides a pipeline model for digital video. Defines the types to efficiently work with video frames and pixel buffers, provides timing and synchronization support, and integrates with [Metal](https://developer.apple.com/documentation/Metal) to ensure efficient video rendering.

[Video Toolbox](https://developer.apple.com/documentation/VideoToolbox)

Provides hardware-accelerated encoding and decoding video data, performs pixel format conversions, and supports advanced video frame processing.
